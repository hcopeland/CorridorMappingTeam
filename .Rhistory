tmp$winter <- dts$winter[i]
return(tmp)
}))
d2 <- do.call(rbind, lapply(1:nrow(dts), function(i){
tmp <- d[d$date >= dts$wint.start[i] & d$date <= dts$wint.end[i],]
if(nrow(tmp)==0){
return(NULL)
}
tmp$winter <- dts$winter[i]
return(tmp)
}))
nrow(d)
nrow(d2)
1:nrow(dts)
i=1
dts$wint.start[i]
head(d)
print(paste0("Your shapefiles has ", nrow(d), " rows."))
if(all(c(idname,datename) %in% names(d)) == FALSE)
stop("There is an issue with the columns in your sequences. See Error 1.")
#reproject to new projection, if necessary
proj <- proj4string(d)
if(proj != out_proj){
d <- spTransform(d, CRS(out_proj))
}
#reduce the dataset to columns of interest
d <- d[,c(idname,datename)]
names(d) <- c("id","date")
#fix the dates  (it is OK to specify GMT, since all dates will be in GMT!)
d$date <- as.POSIXct(strptime(d$date,format = "%Y-%m-%d %H:%M:%S"), tz="GMT")
if(any(is.na(d$date)))
stop("You have a problem with your date column! It does not seem to be a character that was previously in POSIX format.")
d2 <- do.call(rbind, lapply(1:nrow(dts), function(i){
tmp <- d[d$date >= dts$wint.start[i] & d$date <= dts$wint.end[i],]
if(nrow(tmp)==0){
return(NULL)
}
tmp$winter <- dts$winter[i]
return(tmp)
}))
d2 <- do.call(rbind, lapply(1:nrow(dts), function(i){
tmp <- d[d$date >= dts$wint.start[i] & d$date <= dts$wint.end[i],]
if(nrow(tmp)==0){
next
}
tmp$winter <- dts$winter[i]
return(tmp)
}))
d2 <- do.call(rbind, lapply(1:nrow(dts), function(i){
tmp <- d[d$date >= dts$wint.start[i] & d$date <= dts$wint.end[i],]
if(nrow(tmp)==0){
return(NULL)
}
tmp$winter <- dts$winter[i]
return(tmp)
}))
d2 <- do.call(bind, lapply(1:nrow(dts), function(i){
tmp <- d[d$date >= dts$wint.start[i] & d$date <= dts$wint.end[i],]
if(nrow(tmp)==0){
return(NULL)
}
tmp$winter <- dts$winter[i]
return(tmp)
}))
d2 <- lapply(1:nrow(dts), function(i){
tmp <- d[d$date >= dts$wint.start[i] & d$date <= dts$wint.end[i],]
if(nrow(tmp)==0){
return(NULL)
}
tmp$winter <- dts$winter[i]
return(tmp)
})
head(d2)
head(d2[[1]])
head(d2[[2]])
head(d2[[3]])
head(d2[[4]])
head(d2[[5]])
i=1
tmp <- d[d$date >= dts$wint.start[i] & d$date <= dts$wint.end[i],]
nrow(tmp)
d2[sapply(d2, is.null)] <- NULL
d2 <- do.call(rbind, d2)
nrow(d)
nrow(d2)
d <- lapply(1:nrow(dts), function(i){
tmp <- d[d$date >= dts$wint.start[i] & d$date <= dts$wint.end[i],]
if(nrow(tmp)==0){
return(NULL)
}
tmp$winter <- dts$winter[i]
return(tmp)
})
d[sapply(d, is.null)] <- NULL
d <- do.call(rbind, d)
d <- st_read(shpfl_fldr, shpfl_name)
d <- as(d, "Spatial")
print(paste0("Your shapefiles has ", nrow(d), " rows."))
if(all(c(idname,datename) %in% names(d)) == FALSE)
stop("There is an issue with the columns in your sequences. See Error 1.")
#reproject to new projection, if necessary
proj <- proj4string(d)
if(proj != out_proj){
d <- spTransform(d, CRS(out_proj))
}
#reduce the dataset to columns of interest
d <- d[,c(idname,datename)]
names(d) <- c("id","date")
#fix the dates  (it is OK to specify GMT, since all dates will be in GMT!)
d$date <- as.POSIXct(strptime(d$date,format = "%Y-%m-%d %H:%M:%S"), tz="GMT")
if(any(is.na(d$date)))
stop("You have a problem with your date column! It does not seem to be a character that was previously in POSIX format.")
dts
#keep only the data within the date ranges
d$year <- as.numeric(strftime(d$date, format = "%Y", tz = "GMT"))
i=1
tmp <- d[d$date >= dts$wint.start[i] & d$date <= dts$wint.end[i],]
if(nrow(tmp)==0){
return(NULL)
}
tmp$winter <- dts$winter[i]
as.numeric(substr(dts$wint.start[i],6,7)) <6
head(paste0(tmp$id,"_wi", substr(tmp$year,3,4)))
d$month <- as.numeric(strftime(d$date, format = "%m", tz = "GMT"))
d$jul <- as.numeric(strftime(d$date, format = "%j", tz = "GMT"))
d <- lapply(1:nrow(dts), function(i){
tmp <- d[d$date >= dts$wint.start[i] & d$date <= dts$wint.end[i],]
if(nrow(tmp)==0){
return(NULL)
}
tmp$winter <- dts$winter[i]
#create a unique winter period ID (so the year of teh winter means the year at the end of the winter period)
if(as.numeric(substr(dts$wint.start[i],6,7)) <6){ #if you specified start of winter after 1 January, instead of in the previous year
tmp$id_yr <- paste0(tmp$id,"_wi", substr(tmp$year,3,4))
}else{
tmp$id_yr <- paste0(tmp$id,"_wi", substr(ifelse(temp$month>6,temp$year+1,temp$year),3,4))
}
return(tmp)
})
d <- lapply(1:nrow(dts), function(i){
tmp <- d[d$date >= dts$wint.start[i] & d$date <= dts$wint.end[i],]
if(nrow(tmp)==0){
return(NULL)
}
tmp$winter <- dts$winter[i]
#create a unique winter period ID (so the year of teh winter means the year at the end of the winter period)
if(as.numeric(substr(dts$wint.start[i],6,7)) <6){ #if you specified start of winter after 1 January, instead of in the previous year
tmp$id_yr <- paste0(tmp$id,"_wi", substr(tmp$year,3,4))
}else{
tmp$id_yr <- paste0(tmp$id,"_wi", substr(ifelse(tmp$month>6,tmp$year+1,tmp$year),3,4))
}
return(tmp)
})
d[sapply(d, is.null)] <- NULL
d <- do.call(rbind, d)
nrow(d)
head(d)
if(any(is.na(d$id_yr)))
stop("You have an issue. See error 2.")
seqs <- unique(d$id_yr)
source("C:/Users/jmerkle/Documents/GitHub/CorridorMappingTeam/functions/hack.corridors.R")
source("C:/Users/jmerkle/Documents/GitHub/CorridorMappingTeam/functions/create.lns.file.R")
# step 1b. Create a lines files from the sequences.
create.lns.file(seqs_fldr = "C:/Users/jmerkle/Desktop/Mapp2/tab6output/sequences",
out_fldr="C:/Users/jmerkle/Desktop/Mapp2/tab6output/migration_lines",
proj_of_dbfs="+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0")  # this is the proj4string of your data. (should be carried through from previous functions)
lines_fldr = "C:/Users/jmerkle/Desktop/Mapp2/tab6output/migration_lines"
lines_name = "migration_lines"
out_fldr="C:/Users/jmerkle/Desktop/Mapp2/tab6output/hack_corridors"
proj_of_dbfs="+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"
buff=200
cell.size=50
if(all(c("rgdal","foreign","stringr","rgeos","raster") %in% installed.packages()[,1])==FALSE)
stop("You must install the following packages: rgdal, foreign,rgeos, raster, and stringr")
require(rgdal)
require(foreign)
require(stringr)
require(rgeos)
require(raster)
#check the new directories
if(dir.exists(out_fldr)==FALSE){
dir.create(out_fldr)
}
if(length(dir(out_fldr))> 0)
stop("Your out_fldr Has something in it. It should be empty!")
d <- readOGR(lines_fldr, lines_name)
head(d)
lns <- d
rm(d)
ids <- lns$id
ids_unique <- unique(ids)
id_buffs <- do.call(rbind, lapply(1:length(ids_unique), function(i){
tmp <- lns[lns$id %in% ids_unique[i],]
tmpb <- gBuffer(tmp, byid=TRUE, width=buff)
tmpb <- gUnaryUnion(tmpb, id=NULL)
tmpb <- SpatialPolygonsDataFrame(tmpb, data=data.frame(id=ids_unique[i]))
}))
nrow(id_buffs)
plot(id_buffs)
ext <- extent(id_buffs)
grd <- raster(ext)
res(grd) <- cell.size
projection(grd) <- proj4string(id_buffs)
grd <- rasterize(id_buffs[1,], grd, background=0)
for(i in 2:nrow(id_buffs)){
grd <- grd+rasterize(id_buffs[i,], grd, background=0)
}
plot(grd)
writeRaster(grd, filename=paste0(out_fldr, "/popFootprint.img"), format="HFA")
length(ids_unique)
popFootprint <- grd/length(ids_unique)
plot(popFootprint)
corridor_percents=c(10, 20)
min_area = 20000
simplify = TRUE
tolerance = 25 # unites are metersbuff=200
cell.size=50
corridor_percents <- corridor_percents[(corridor_percents/100) > (2/numb_ids)]
numb_ids <- length(ids_unique)
corridor_percents <- corridor_percents[(corridor_percents/100) > (2/numb_ids)]
corridor_percents
thresholdQuantiles = c(0, c(2/numb_ids)-0.0001, (corridor_percents/100)-0.0001, .99)  #this starts with low 1 or more, then low 2 or more, then the other percents
thresholdQuantiles
thresholdQuantiles_names <- c(1,2,corridor_percents)
print(paste0("GRIDCODEs of ", paste(thresholdQuantiles_names,collapse = ", "),
" represent the following ranges of individuals: 0 - ",
paste(floor(thresholdQuantiles*numb_ids)+1,collapse=" - "),"."))
# compute the contours
classifiedRaster = cut(popFootprint,breaks=thresholdQuantiles)
# extract the contours as polygons
polygonList = rasterToPolygons(classifiedRaster,dissolve=T)
lines_fldr = "C:/Users/jmerkle/Desktop/Mapp2/tab6output/migration_lines"
lines_name = "migration_lines"
out_fldr="C:/Users/jmerkle/Desktop/Mapp2/tab6output/hack_corridors"
corridor_percents=c(10, 20)
min_area = 20000
simplify = TRUE
tolerance = 25 # unites are metersbuff=200
cell.size=500
#check the new directories
if(dir.exists(out_fldr)==FALSE){
dir.create(out_fldr)
}
if(length(dir(out_fldr))> 0)
stop("Your out_fldr Has something in it. It should be empty!")
#load up teh data into a single database
fls <- dir(seqs_fldr)
print(paste0("You have ", length(fls), " sequences."))
d <- do.call(rbind, lapply(1:length(fls), function(i){
db <- read.dbf(paste(seqs_fldr, fls[i],sep="/"), as.is=TRUE)
db$mig <- sub(".dbf","",fls[i])
return(db)
}))
#check and make sure the columns are correct.
if(all(c("date","x","y") %in% names(d)) == FALSE)
stop("There is an issue with the columns in your sequences. See Error 2.")
lines_fldr = "C:/Users/jmerkle/Desktop/Mapp2/tab6output/migration_lines"
lines_name = "migration_lines"
out_fldr="C:/Users/jmerkle/Desktop/Mapp2/tab6output/hack_corridors"
corridor_percents=c(10, 20)
min_area = 20000
simplify = TRUE
tolerance = 25 # unites are metersbuff=200
cell.size=500
#check the new directories
if(dir.exists(out_fldr)==FALSE){
dir.create(out_fldr)
}
if(length(dir(out_fldr))> 0)
stop("Your out_fldr Has something in it. It should be empty!")
lns <- readOGR(lines_fldr, lines_name)
ids <- lns$id
ids_unique <- unique(ids)
id_buffs <- do.call(rbind, lapply(1:length(ids_unique), function(i){
tmp <- lns[lns$id %in% ids_unique[i],]
tmpb <- gBuffer(tmp, byid=TRUE, width=buff)
tmpb <- gUnaryUnion(tmpb, id=NULL)
tmpb <- SpatialPolygonsDataFrame(tmpb, data=data.frame(id=ids_unique[i]))
}))
buff=200
id_buffs <- do.call(rbind, lapply(1:length(ids_unique), function(i){
tmp <- lns[lns$id %in% ids_unique[i],]
tmpb <- gBuffer(tmp, byid=TRUE, width=buff)
tmpb <- gUnaryUnion(tmpb, id=NULL)
tmpb <- SpatialPolygonsDataFrame(tmpb, data=data.frame(id=ids_unique[i]))
}))
nrow(id_buffs)
ext <- extent(id_buffs)
grd <- raster(ext)
res(grd) <- cell.size
projection(grd) <- proj4string(id_buffs)
grd <- rasterize(id_buffs, grd fun="count", background=o)
grd <- rasterize(id_buffs, grd, fun="count", background=o)
grd <- rasterize(id_buffs[1,], grd, background=0)
for(i in 2:nrow(id_buffs)){
grd <- grd+rasterize(id_buffs[i,], grd, background=0)
}
plot(popFootprint)
popFootprint <- grd
rm(grd)
plot(popFootprint)
writeRaster(popFootprint, filename=paste0(out_fldr, "/popFootprint.img"), format="HFA")
numb_ids <- length(ids_unique)
popFootprint <- popFootprint/numb_ids
plot(popFootprint)
# plot(popFootprint)
corridor_percents <- corridor_percents[(corridor_percents/100) > (2/numb_ids)]
thresholdQuantiles = c(0, c(2/numb_ids)-0.0001, (corridor_percents/100)-0.0001, .99)  #this starts with low 1 or more, then low 2 or more, then the other percents
thresholdQuantiles_names <- c(1,2,corridor_percents)
print(paste0("GRIDCODEs of ", paste(thresholdQuantiles_names,collapse = ", "),
" represent the following ranges of individuals: 0 - ",
paste(floor(thresholdQuantiles*numb_ids)+1,collapse=" - "),"."))
# compute the contours
classifiedRaster = cut(popFootprint,breaks=thresholdQuantiles)
# extract the contours as polygons
polygonList = rasterToPolygons(classifiedRaster,dissolve=T)
# Methods - Data Munging -------------------------------------------
# reads and updates the uniqueID global variable to assign
# identifiers to each polygon (unique across shapefiles).
makeIndividualPolygons<-function(multiPoly,attr,minArea=0){
#polygon.list = polygonList.acc[[1]]@polygons[[1]]@Polygons
polygon.list = multiPoly@Polygons
area = sapply(polygon.list,function(x) x@area)
isHole = sapply(polygon.list,function(x) x@hole)
area[which(isHole)]=-1*area[which(isHole)]
holeIndices = which(!isHole)
diffArea=numeric()
startID=uniqueID+1
if(length(holeIndices)>1){
polygon.list.wrap = lapply(1:(length(holeIndices)-1),function(x) {
uniqueID<<-uniqueID+1
temp = holeIndices[x]:(holeIndices[x+1]-1)
diffArea[x]<<-sum(area[temp])
Polygons(polygon.list[temp],uniqueID)
})
}else{
polygon.list.wrap = list()
}
uniqueID<<-uniqueID+1
temp = holeIndices[length(holeIndices)]:length(polygon.list)
polygon.list.wrap[[length(polygon.list.wrap)+1]] = Polygons(polygon.list[temp],uniqueID)
diffArea[length(holeIndices)]=sum(area[temp])
endID=uniqueID
id=startID:endID
temp = which(diffArea>minArea)
diffArea=diffArea[temp]
polygon.list.wrap=polygon.list.wrap[temp]
id=id[temp]
#sum(sapply(1:length(polygon.list.wrap), function(x) sapply(polygon.list.wrap[[x]]@Polygons, function(y) y@hole)))
#sapply(polygon.list.wrap[[2]]@Polygons,function(x) x@hole)
#print(sapply(1:length(index), function(x) sapply(polygon.list.wrap[[x]]@Polygons, function(y) y@hole=isHole[x])))
attrTable = data.frame(ID=id, GRIDCODE=attr)
row.names(attrTable) = id
x=SpatialPolygonsDataFrame(SpatialPolygons(polygon.list.wrap), data=attrTable)
#print(length(sapply(sapply(x@polygons,function(y) sapply(y@Polygons, function(z) z@hole)),sum)))
#print(sum(sapply(sapply(x@polygons,function(y) sapply(y@Polygons, function(z) z@hole)),sum)))
x
}
# NOTE: The simplification is set to preserve topology; although not
# strictly necessary for visualizaiton, this avoids the scenario where
# large polygons were mysteriously getting excluded from the output.
# each polygon contains only the area within a single cut
# join the polygons with any in higher cuts (cumulative area for each level)
n=length(polygonList@polygons)
polygonList.acc <- lapply(1:n, function(x){
idList = c(rep(NA,x-1),rep(1,n-x+1))
#print(idList)
unionSpatialPolygons(SpatialPolygons(polygonList@polygons),idList)
})
??unionSpatialPolygons
require(maptools)
# NOTE: The simplification is set to preserve topology; although not
# strictly necessary for visualizaiton, this avoids the scenario where
# large polygons were mysteriously getting excluded from the output.
# each polygon contains only the area within a single cut
# join the polygons with any in higher cuts (cumulative area for each level)
n=length(polygonList@polygons)
polygonList.acc <- lapply(1:n, function(x){
idList = c(rep(NA,x-1),rep(1,n-x+1))
#print(idList)
unionSpatialPolygons(SpatialPolygons(polygonList@polygons),idList)
})
# line simplification
if(simplify==TRUE){
polygonList.acc <- lapply(polygonList.acc,thinnedSpatialPoly,
tolerance,topologyPreserve=T)
}
# split up multi-polygons into separate polygons
uniqueID<-0 # reset before calling makeIndividual polygons
spDataFrame.list <- sapply(1:length(polygonList.acc),function(x) makeIndividualPolygons(
polygonList.acc[[x]]@polygons[[1]],thresholdQuantiles_names[x],min_area))
for(x in 1:length(spDataFrame.list)){
proj4string(spDataFrame.list[[x]])<-proj_of_ascs
}
for(x in 1:length(spDataFrame.list)){
proj4string(spDataFrame.list[[x]])<-proj4string(id_buffs)
}
bigData.df = data.frame(ID=numeric(), GRIDCODE=numeric())
bigData.sp = list()
# accumulate the data
for(i in 1:length(spDataFrame.list)){
bigData.df = rbind(bigData.df,spDataFrame.list[[i]]@data)
bigData.sp = append(bigData.sp,spDataFrame.list[[i]]@polygons)
}
# create the spatial data frame with a projection (same as above)
bigData = SpatialPolygonsDataFrame(SpatialPolygons(bigData.sp),bigData.df)
proj4string(bigData) <- proj_of_ascs
proj4string(bigData) <- proj4string(id_buffs)
bigData <- aggregate(bigData, "GRIDCODE")   #dissolve the polygons based on the gridcode
bigData <- bigData[order(bigData$GRIDCODE),]
plot(bigData)
head(bigData)
# write to a shapefile
writeOGR(bigData,out_fldr,"corridors", driver="ESRI Shapefile")
source("C:/Users/jmerkle/Documents/GitHub/CorridorMappingTeam/functions/create.seqs.R")
source("C:/Users/jmerkle/Documents/GitHub/CorridorMappingTeam/functions/create.BBs.R")
source("C:/Users/jmerkle/Documents/GitHub/CorridorMappingTeam/functions/create.BB.avgs.R")
source("C:/Users/jmerkle/Documents/GitHub/CorridorMappingTeam/functions/create.corridors.stopovers.R")
source("C:/Users/jmerkle/Documents/GitHub/CorridorMappingTeam/functions/create.lns.file.R")
PopUD_asc = "C:/Users/jmerkle/Desktop/Mapp2/tab6output/UDs_pop/averageUD.asc"   #this is the file path for the POPud ascii file
PopFootprint_asc = "C:/Users/jmerkle/Desktop/Mapp2/tab6output/Footprints_pop/popFootprint.asc"  #this is the file path for the POfootprint ascii file
pop_BBs_fldr = "C:/Users/jmerkle/Desktop/Mapp2/tab6output/Footprints_pop"   #this is the pop BBs folder
out_fldr = "C:/Users/jmerkle/Desktop/Mapp2/tab6output/final_products"     #this is an empty folder where you want the results to be saved
stopover_percent=90 #this is the contour level for stopovers
corridor_percents=c(10, 20)  #the corridor percents that are provided (these are in addition to 1 or more, and 2 or more corridors)
min_area = 20000 #if there are polygons smaller than this (in squared meters), they will be removed
simplify = TRUE #should polygons be simplified?
tolerance = 25 # how to polygons are simplified (unites are meters)
proj_of_ascs="+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"
#manage packages
if(all(c("raster","BBMM","igraph","rgdal","rgeos","maptools") %in% installed.packages()[,1])==FALSE)
stop("You must install the following packages: raster,igraph, rgeos, BBMM, maptools, and rgdal.")
require(BBMM)
#check the new directories
if(dir.exists(out_fldr)==FALSE){
dir.create(out_fldr)
}
if(length(dir(out_fldr))> 0)
stop("Your out_fldr Has something in it. It should be empty!")
stopovers <- raster(PopUD_asc)
projection(stopovers) <- proj_of_ascs
require(raster)
#check the new directories
if(dir.exists(out_fldr)==FALSE){
dir.create(out_fldr)
}
if(length(dir(out_fldr))> 0)
stop("Your out_fldr Has something in it. It should be empty!")
stopovers <- raster(PopUD_asc)
projection(stopovers) <- proj_of_ascs
bb <- list("Brownian motion variance" = 0, "x" = coordinates(stopovers)[,1], "y" = coordinates(stopovers)[,2], "probability" = values(stopovers))
qtl <- bbmm.contour(bb, levels = c(100-stopover_percent), plot = FALSE)
qtl
qtl <- bbmm.contour(bb, levels = stopover_percent, plot = FALSE)
qtl
qtl <- bbmm.contour(bb, levels = stopover_percent, plot = TRUE)
cutoff <- sort(bb$probability, decreasing=TRUE)
vlscsum <- cumsum(cutoff)
cutoff <- cutoff[vlscsum > .1][1]
cutoff
cutoff <- cutoff[vlscsum > .9][1]
cutoff
cutoff <- sort(bb$probability, decreasing=TRUE)
vlscsum <- cumsum(cutoff)
cutoff <- cutoff[vlscsum > .9][1]
cutoff
head(vlscsum)
head(vlscsum, 20)
head(cutoff, 20)
cutoff <- sort(bb$probability, decreasing=TRUE)
head(cutoff, 20)
stopover_percent/100
cutoff <- sort(bb$probability, decreasing=TRUE)
vlscsum <- cumsum(cutoff)
cutoff <- cutoff[vlscsum > stopover_percent/100][1]
cutoff
stopovers <- raster(PopUD_asc)
projection(stopovers) <- proj_of_ascs
stopover_percent
cutoff <- sort(values(stopovers), decreasing=TRUE)
vlscsum <- cumsum(cutoff)
cutoff <- cutoff[vlscsum > stopover_percent/100][1]
cutoff
# bb <- list("Brownian motion variance" = 0, "x" = coordinates(stopovers)[,1], "y" = coordinates(stopovers)[,2], "probability" = values(stopovers))
# qtl <- bbmm.contour(bb, levels = stopover_percent, plot = FALSE)
stopovers <- reclassify(stopovers, rcl=matrix(c(-1,cutoff,NA,cutoff,Inf,1),2,3, byrow=T))
plot(stopovers)
stopover_percent=10
stopovers <- raster(PopUD_asc)
projection(stopovers) <- proj_of_ascs
cutoff <- sort(values(stopovers), decreasing=TRUE)
vlscsum <- cumsum(cutoff)
cutoff <- cutoff[vlscsum > stopover_percent/100][1]
# bb <- list("Brownian motion variance" = 0, "x" = coordinates(stopovers)[,1], "y" = coordinates(stopovers)[,2], "probability" = values(stopovers))
# qtl <- bbmm.contour(bb, levels = stopover_percent, plot = FALSE)
stopovers <- reclassify(stopovers, rcl=matrix(c(-1,cutoff,NA,cutoff,Inf,1),2,3, byrow=T))
plot(stopovers)
table(values(stopovers))
bb <- list("Brownian motion variance" = 0, "x" = coordinates(stopovers)[,1], "y" = coordinates(stopovers)[,2], "probability" = values(stopovers))
as(bb, "BBMM")
as(bb, "bbmm")
qtl <- bbmm.contour(bb, levels = c(50, 60, 70), plot = FALSE)
qtl
stopovers <- raster(PopUD_asc)
projection(stopovers) <- proj_of_ascs
bb <- list("Brownian motion variance" = 0, "x" = coordinates(stopovers)[,1], "y" = coordinates(stopovers)[,2], "probability" = values(stopovers))
qtl <- bbmm.contour(bb, levels = c(50, 60, 70), plot = FALSE)
qtl
plot(stopovers)
hist(value(stopovers))
hist(values(stopovers))
as(bb, "bbmm")
